#============
# RANDOM FOREST POUR LA REGRESSION
# Objectif : expliquer et prédire les valeurs de plusieurs features
#============

#============ description des données
"""
Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return 
back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return 
back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of 
over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, 
environmental and health issues. 

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by
these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration
of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into
a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important
events in the city could be detected via monitoring these data.

=========================================
Associated tasks
=========================================

	- Regression: 
		Predication of bike rental count hourly or daily based on the environmental and seasonal settings.
	
	- Event and Anomaly Detection:  
		Count of rented bikes are also correlated to some events in the town which easily are traceable via search engines.
		For instance, query like "2012-10-30 washington d.c." in Google returns related results to Hurricane Sandy. Some of the important events are 
		identified in [1]. Therefore the data can be used for validation of anomaly or event detection algorithms as well.
        

=========================================
Dataset characteristics
=========================================	
Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv
	
	- instant: record index
	- dteday : date
	- season : season (1:springer, 2:summer, 3:fall, 4:winter)
	- yr : year (0: 2011, 1:2012)
	- mnth : month ( 1 to 12)
	- hr : hour (0 to 23)
	- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
	- weekday : day of the week
	- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
	+ weathersit : 
		- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
		- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
		- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
		- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
	- temp : Normalized temperature in Celsius. The values are divided to 41 (max)
	- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)
	- hum: Normalized humidity. The values are divided to 100 (max)
	- windspeed: Normalized wind speed. The values are divided to 67 (max)
	- casual: count of casual users
	- registered: count of registered users
	- cnt: count of total rental bikes including both casual and registered
"""

#============ vérifier la propreté du code
# pip install flake8
# invoke flake8 (bash) : flake8

#============ chargement des bibliothèques
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV 

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

from sklearn.ensemble import RandomForestRegressor

#============ importation des données
path = "https://raw.githubusercontent.com/JoyceMbiguidi/data/main/bike_hour.csv"
raw_df = pd.read_csv(path, sep = ";")

#============ copie du dataset brut
df = raw_df
df.head()

#============ vérification des types
df.dtypes

#============ afficher la dimension
print(df.shape)

#============ check des valeurs manquantes
df.isna().sum()

"""
Problematique : on veut expliquer les facteurs qui impactent la location de vélo et prédire les ventes horaires
"""

#============ data wrangling
pre_dropped = ["dteday", "casual", "registered", "instant"]
df = df.drop(pre_dropped, axis=1)

#============ quick data viz
df.hist(rwidth=0.9, figsize=(40, 30))
plt.tight_layout()
plt.show()

#============ matrice de correlation
import seaborn as sns
import matplotlib.pyplot as plt
corr_matrix = df.corr().round(2)

mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # Generate a mask for the upper triangle
plt.figure(figsize = (16,10))
sns.heatmap(corr_matrix, cmap = 'RdBu_r', mask = mask, annot = True)
plt.show()

#============ data wrangling
""" on retire les variables non corrélées à la variable d'interet cnt
"""
dropped = ["windspeed", "atemp", "workingday", "weekday", "yr"]
df = df.drop(dropped, axis=1)

""" dummy"""
dummy = ["season", "mnth", "hr", "holiday", "weathersit"]
dummy_df = pd.get_dummies(df[dummy].astype("category"), drop_first=True)
dummy_df.head()

"""jeu de données final"""
dropped = ["season", "mnth", "holiday", "weathersit", "hr"]
df = pd.concat((df.drop(dropped, axis=1), dummy_df), axis=1)
df.head()

#============ variables explicatives
x = df.drop(['cnt'], axis=1).to_numpy()
x.shape

#============ variable à expliquer
y = df['cnt'].to_numpy()
y.shape

#============ séparation des données : train - test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle = True, random_state = 42)

#============ MODELE 1
model1 = RandomForestRegressor()
model1.fit(x_train, y_train)

#============ prediction sur le jeu de test
y_pred = model1.predict(x_test)

#============ R2 coefficient de determination
r2 = r2_score(y_test, y_pred).round(3)
print(r2)

#============ RMSE (Root Mean Square Error)
rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))
print("\nRMSE: ", rmse)

#============ importance des variables
# dictionnaire des features + importance des variables
feat_dict= {}
for col, val in sorted(zip(df.drop('cnt', axis = 1).columns, model1.feature_importances_), key=lambda x:x[1], reverse=True):
  feat_dict[col]=val

# on convertit le dictionnaire en dataframe
feat_df = pd.DataFrame({'Feature':feat_dict.keys(),'Importance':feat_dict.values()})
feat_df



#============
# Hyperparameter tunning
#============
"""attention traitement long"""

hyperparameters = {'max_depth':range(3,21),
                   'n_estimators': [10, 20, 30, 40, 50, 60, 80, 100, 120, 150],
                   'n_jobs': [-1]}

model2 = RandomForestRegressor(random_state = 42, n_estimators = 5)

gridsearchcv = GridSearchCV(model2, hyperparameters, cv = 5)

gridsearchcv.fit(x_train,y_train)

gridsearchcv.best_params_

#============ nouveau modele
model_rf = RandomForestRegressor(random_state = 42, n_estimators = 120, max_depth = 20)
model_rf.fit(x_train, y_train)

y_predict_train = model_rf.predict(x_train)
y_predict_test = model_rf.predict(x_test)

r_score_train = r2_score(y_train, y_predict_train)
r_score_test = r2_score(y_test, y_predict_test)

print(r_score_train)
print(r_score_test)

#============ importance des variables
# dictionnaire des features + importance des variables
feat_dict= {}
for col, val in sorted(zip(df.drop('cnt', axis = 1).columns, model1.feature_importances_), key=lambda x:x[1], reverse=True):
  feat_dict[col]=val

# on convertit le dictionnaire en dataframe
feat_df = pd.DataFrame({'Feature':feat_dict.keys(),'Importance':feat_dict.values()})
feat_df

"""les variables les plus pertinentes sont peu interpretables"""

#============ modele 3
# Disrétisation des variables
df["temp_disc"] = pd.cut(df.temp, bins = 2)
df["hum_disc"] = pd.cut(df.hum, bins = 2)

""" dummy"""
dummy = ["temp_disc", "hum_disc"]
dummy_df = pd.get_dummies(df[dummy].astype("category"), drop_first=True)
dummy_df.head()

"""jeu de données final"""
dropped = ["temp", "hum", "temp_disc", "hum_disc"]
df = pd.concat((df.drop(dropped, axis=1), dummy_df), axis=1)
df.head()

#============ variables explicatives
x = df.drop(['cnt'], axis=1).to_numpy()
x.shape

#============ variable à expliquer
y = df['cnt'].to_numpy()
y.shape

#============ séparation des données : train - test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle = True, random_state = 42)

#============ MODELE 3 initialisation
model3 = RandomForestRegressor()
model3.fit(x_train, y_train)

#============ prediction sur le jeu de test
y_pred = model3.predict(x_test)

# tuning des hyper parametres
hyperparameters = {'max_depth':range(3,21),
                   'n_estimators': [10, 20, 30, 40, 50, 60, 80, 100, 120, 150],
                   'n_jobs': [-1]}

model2 = RandomForestRegressor(random_state = 42, n_estimators = 5)

gridsearchcv = GridSearchCV(model3, hyperparameters, cv = 5)

gridsearchcv.fit(x_train,y_train)

gridsearchcv.best_params_


#============ dernier modele
model_rf = RandomForestRegressor(random_state = 42, n_estimators = 120, max_depth = 20)
model_rf.fit(x_train, y_train)

y_predict_train = model_rf.predict(x_train)
y_predict_test = model_rf.predict(x_test)

r_score_train = r2_score(y_train, y_predict_train)
r_score_test = r2_score(y_test, y_predict_test)

print(r_score_train)
print(r_score_test)

#============ importance des variables
# dictionnaire des features + importance des variables
feat_dict= {}
for col, val in sorted(zip(df.drop('cnt', axis = 1).columns, model1.feature_importances_), key=lambda x:x[1], reverse=True):
  feat_dict[col]=val

# on convertit le dictionnaire en dataframe
feat_df = pd.DataFrame({'Feature':feat_dict.keys(),'Importance':feat_dict.values()})
feat_df
